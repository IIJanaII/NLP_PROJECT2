{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6684a5d9-5402-4802-8cec-2e1e8a8c3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\meama\\Documents\\A5\\AAI_Machine_Learning_for_NLP\\Project2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becb590c-2c81-4e3f-b618-ea2f030e4be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "      <th>average_score</th>\n",
       "      <th>number_votes</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>email</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>location</th>\n",
       "      <th>language</th>\n",
       "      <th>language_com2</th>\n",
       "      <th>description_trad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Handyman in Paris</td>\n",
       "      <td>https://www.yelp.fr/biz/american-handyman-in-p...</td>\n",
       "      <td>Petits travaux à domicile Électricien Plombier...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>When you are in the US and your problem is in ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>americanhandymaninparis.fr</td>\n",
       "      <td>06 12 32 04 39</td>\n",
       "      <td>no location</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Small home jobs Electrician Plumber Appliance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Handyman in Paris</td>\n",
       "      <td>https://www.yelp.fr/biz/american-handyman-in-p...</td>\n",
       "      <td>Petits travaux à domicile Électricien Plombier...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>If you are wondering if you should call him, I...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>americanhandymaninparis.fr</td>\n",
       "      <td>06 12 32 04 39</td>\n",
       "      <td>no location</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Small home jobs Electrician Plumber Appliance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American Handyman in Paris</td>\n",
       "      <td>https://www.yelp.fr/biz/american-handyman-in-p...</td>\n",
       "      <td>Petits travaux à domicile Électricien Plombier...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Have had multiple plumbing issues in my apartm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>americanhandymaninparis.fr</td>\n",
       "      <td>06 12 32 04 39</td>\n",
       "      <td>no location</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Small home jobs Electrician Plumber Appliance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Handyman in Paris</td>\n",
       "      <td>https://www.yelp.fr/biz/american-handyman-in-p...</td>\n",
       "      <td>Petits travaux à domicile Électricien Plombier...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Very efficient and quick! Maxim responded to m...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>americanhandymaninparis.fr</td>\n",
       "      <td>06 12 32 04 39</td>\n",
       "      <td>no location</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Small home jobs Electrician Plumber Appliance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Handyman in Paris</td>\n",
       "      <td>https://www.yelp.fr/biz/american-handyman-in-p...</td>\n",
       "      <td>Petits travaux à domicile Électricien Plombier...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>We had furniture delivered when we moved to Pa...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>americanhandymaninparis.fr</td>\n",
       "      <td>06 12 32 04 39</td>\n",
       "      <td>no location</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Small home jobs Electrician Plumber Appliance ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0  American Handyman in Paris   \n",
       "1  American Handyman in Paris   \n",
       "2  American Handyman in Paris   \n",
       "3  American Handyman in Paris   \n",
       "4  American Handyman in Paris   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.yelp.fr/biz/american-handyman-in-p...   \n",
       "1  https://www.yelp.fr/biz/american-handyman-in-p...   \n",
       "2  https://www.yelp.fr/biz/american-handyman-in-p...   \n",
       "3  https://www.yelp.fr/biz/american-handyman-in-p...   \n",
       "4  https://www.yelp.fr/biz/american-handyman-in-p...   \n",
       "\n",
       "                                         description  average_score  \\\n",
       "0  Petits travaux à domicile Électricien Plombier...            5.0   \n",
       "1  Petits travaux à domicile Électricien Plombier...            5.0   \n",
       "2  Petits travaux à domicile Électricien Plombier...            5.0   \n",
       "3  Petits travaux à domicile Électricien Plombier...            5.0   \n",
       "4  Petits travaux à domicile Électricien Plombier...            5.0   \n",
       "\n",
       "   number_votes                                            comment  score  \\\n",
       "0             6  When you are in the US and your problem is in ...    5.0   \n",
       "1             6  If you are wondering if you should call him, I...    5.0   \n",
       "2             6  Have had multiple plumbing issues in my apartm...    5.0   \n",
       "3             6  Very efficient and quick! Maxim responded to m...    5.0   \n",
       "4             6  We had furniture delivered when we moved to Pa...    5.0   \n",
       "\n",
       "                        email    phone_number     location language  \\\n",
       "0  americanhandymaninparis.fr  06 12 32 04 39  no location       en   \n",
       "1  americanhandymaninparis.fr  06 12 32 04 39  no location       en   \n",
       "2  americanhandymaninparis.fr  06 12 32 04 39  no location       en   \n",
       "3  americanhandymaninparis.fr  06 12 32 04 39  no location       en   \n",
       "4  americanhandymaninparis.fr  06 12 32 04 39  no location       en   \n",
       "\n",
       "  language_com2                                   description_trad  \n",
       "0            en  Small home jobs Electrician Plumber Appliance ...  \n",
       "1            en  Small home jobs Electrician Plumber Appliance ...  \n",
       "2            en  Small home jobs Electrician Plumber Appliance ...  \n",
       "3            en  Small home jobs Electrician Plumber Appliance ...  \n",
       "4            en  Small home jobs Electrician Plumber Appliance ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\meama\\Documents\\A5\\AAI_Machine_Learning_for_NLP\\Project2\\df_new_format_trad.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0169d7-c647-4a35-8acd-f854d17e8b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34023a1-da6f-45fb-abf2-09adb49611d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, dtype('O'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comment.fillna(\"\", inplace=True)\n",
    "df.comment=df.comment.astype(str)\n",
    "df.comment.isna().sum(), df.comment.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439648eb-3ba6-418f-ba42-5d88c235011d",
   "metadata": {},
   "source": [
    "## T5 summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be131bcd-c879-49eb-90f1-25d2d333c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a179b1a-7717-4203-9c56-a10204c752bd",
   "metadata": {},
   "source": [
    "## Pre-trained1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17049e02-ad12-42f5-b5df-60e6ddeb1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_name = r\"C:\\Users\\meama\\Documents\\A5\\AAI_Machine_Learning_for_NLP\\Project2\\t5_abs_sum\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746b1090-6eb0-4058-9186-b2ba670af74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(review_text):\n",
    "    if review_text==None :\n",
    "        return review_text\n",
    "    # Tokenize the text for T5 input\n",
    "    input_ids = tokenizer(review_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(input_ids['input_ids'], max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Convert the summary token ids to text\n",
    "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982fad0-5d32-41c9-a688-40cd198dbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "businesses = df['name'].unique()\n",
    "\n",
    "# Initialize a dictionary to store summaries\n",
    "summaries = {}\n",
    "\n",
    "# Loop through each unique business\n",
    "for business_name in businesses:\n",
    "    # Filter comments for the current business\n",
    "    business_comments = df[df['name'] == business_name]['comment'].tolist()\n",
    "\n",
    "    # Check if there are comments for the business\n",
    "    if not business_comments:\n",
    "        summaries[business_name] = \"No comments available\"\n",
    "        continue\n",
    "\n",
    "    # Concatenate all comments into a single text\n",
    "    combined_comments = \" \".join(business_comments)\n",
    "\n",
    "    # Generate a summary using the BERT model\n",
    "    #summary = model(combined_comments, min_length=0, max_length=100)  \n",
    "    summary = getSummary(combined_comments)\n",
    "    summaries[business_name] = summary\n",
    "\n",
    "# Store the summaries in your DataFrame\n",
    "df['comment_summary'] = df['name'].map(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefdc75-8779-4058-8728-1fb056bf9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\meama\\Documents\\A5\\AAI_Machine_Learning_for_NLP\\Project2\\df_nf_summarized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afeb0a8-b92b-46e1-a3b2-d6e590a86e4a",
   "metadata": {},
   "source": [
    "## Pre-trained2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f363ba37-29d4-425b-8023-38a37fd31e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\meama\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1564: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "\n",
    "def summarize(text, max_length=150):\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
    "\n",
    "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "  return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd71e43-c04c-4d05-a9fb-ee29f8be402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(df[\\'name\\'].unique().shape)\\n# Assuming your DataFrame is named \\'df\\'\\nbusinesses = df[\\'name\\'].unique()\\n\\n# Initialize a dictionary to store summaries\\nsummaries = {}\\n\\n# Loop through each unique business\\nfor business_name in businesses:\\n    # Filter comments for the current business\\n    business_comments = df[df[\\'name\\'] == business_name][\\'comment\\'].tolist()\\n\\n    # Check if there are comments for the business\\n    if not business_comments:\\n        summaries[business_name] = \"No comments available\"\\n        continue\\n\\n    # Concatenate all comments into a single text\\n    combined_comments = \" \".join(business_comments)\\n\\n    # Generate a summary using the BERT model\\n    #summary = model(combined_comments, min_length=0, max_length=100)  \\n    summary = summarize(combined_comments)\\n    summaries[business_name] = summary\\n\\n# Store the summaries in your DataFrame\\ndf[\\'comment_summary\\'] = df[\\'name\\'].map(summaries)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(df['name'].unique().shape)\n",
    "# Assuming your DataFrame is named 'df'\n",
    "businesses = df['name'].unique()\n",
    "\n",
    "# Initialize a dictionary to store summaries\n",
    "summaries = {}\n",
    "\n",
    "# Loop through each unique business\n",
    "for business_name in businesses:\n",
    "    # Filter comments for the current business\n",
    "    business_comments = df[df['name'] == business_name]['comment'].tolist()\n",
    "\n",
    "    # Check if there are comments for the business\n",
    "    if not business_comments:\n",
    "        summaries[business_name] = \"No comments available\"\n",
    "        continue\n",
    "\n",
    "    # Concatenate all comments into a single text\n",
    "    combined_comments = \" \".join(business_comments)\n",
    "\n",
    "    # Generate a summary using the BERT model\n",
    "    #summary = model(combined_comments, min_length=0, max_length=100)  \n",
    "    summary = summarize(combined_comments)\n",
    "    summaries[business_name] = summary\n",
    "\n",
    "# Store the summaries in your DataFrame\n",
    "df['comment_summary'] = df['name'].map(summaries)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a26e996-13c4-4faa-b0e4-0b56f4b8c8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def summarize(text, max_length=150):\\n    # Break the text into chunks\\n    chunk_size = 400  # Adjust as needed\\n    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\\n\\n    summaries = []\\n    for chunk in chunks:\\n        input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", add_special_tokens=True)\\n        generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\n        chunk_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\n        summaries.append(chunk_summary)\\n\\n    return \" \".join(summaries)\\n\\nprint(df[\\'name\\'].unique().shape)\\nbusinesses = df[\\'name\\'].unique()[:2]\\n\\nsummaries = {}\\nfor i, business_name in enumerate(businesses):\\n    print(i, end=\\'\\r\\')\\n    business_comments = df[df[\\'name\\'] == business_name][\\'comment\\'].tolist()\\n    if not business_comments:\\n        summaries[business_name] = \"No comments available\"\\n        continue\\n\\n    combined_comments = \" \".join(business_comments)\\n    summary = summarize(combined_comments)\\n    summaries[business_name] = summary\\n\\ndf[\\'comment_summary\\'] = df[\\'name\\'].map(summaries)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def summarize(text, max_length=150):\n",
    "    # Break the text into chunks\n",
    "    chunk_size = 400  # Adjust as needed\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
    "        chunk_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        summaries.append(chunk_summary)\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "print(df['name'].unique().shape)\n",
    "businesses = df['name'].unique()[:2]\n",
    "\n",
    "summaries = {}\n",
    "for i, business_name in enumerate(businesses):\n",
    "    print(i, end='\\r')\n",
    "    business_comments = df[df['name'] == business_name]['comment'].tolist()\n",
    "    if not business_comments:\n",
    "        summaries[business_name] = \"No comments available\"\n",
    "        continue\n",
    "\n",
    "    combined_comments = \" \".join(business_comments)\n",
    "    summary = summarize(combined_comments)\n",
    "    summaries[business_name] = summary\n",
    "\n",
    "df['comment_summary'] = df['name'].map(summaries)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cec6e-85cc-4c28-926a-1e396d067972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_business(business_name):\n",
    "    business_comments = df[df['name'] == business_name]['comment'].tolist()\n",
    "    if not business_comments:\n",
    "        return business_name, \"No comments available\"\n",
    "\n",
    "    combined_comments = \" \".join(business_comments)\n",
    "    summary = summarize(combined_comments)\n",
    "    return business_name, summary\n",
    "\n",
    "# Use multi-threading for concurrent processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_business, business_name) for business_name in businesses]\n",
    "    summaries = {business_name: summary for business_name, summary in [future.result() for future in futures]}\n",
    "\n",
    "df['comment_summary'] = df['name'].map(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23aa752d-a3fe-4f73-a409-8f59cdae2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1999/1999 [3:17:08<00:00,  5.92s/it]  \n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "businesses = df['name'].unique()\n",
    "\n",
    "def process_business(business_name):\n",
    "    business_comments = df[df['name'] == business_name]['comment'].tolist()\n",
    "    if not business_comments:\n",
    "        return business_name, \"No comments available\"\n",
    "\n",
    "    combined_comments = \" \".join(business_comments)\n",
    "    summary = summarize(combined_comments)\n",
    "    return business_name, summary\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Prepare futures and wrap them with tqdm for progress display\n",
    "    futures = [executor.submit(process_business, business_name) for business_name in businesses]\n",
    "    results = [future.result() for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures))]\n",
    "\n",
    "# Extracting summaries from results\n",
    "summaries = dict(results)\n",
    "\n",
    "# Mapping summaries to the dataframe\n",
    "df['comment_summary'] = df['name'].map(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957f2fa7-d9c8-417a-8672-03c22884ce41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       You can trust them, perfect English is spoken,...\n",
       "1       You can trust them, perfect English is spoken,...\n",
       "2       You can trust them, perfect English is spoken,...\n",
       "3       You can trust them, perfect English is spoken,...\n",
       "4       You can trust them, perfect English is spoken,...\n",
       "                              ...                        \n",
       "6689                                                  NaN\n",
       "6690                                                  NaN\n",
       "6691                                                  NaN\n",
       "6692                                                  NaN\n",
       "6693                                                  NaN\n",
       "Name: comment_summary, Length: 6694, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c4eaa-3f66-49b3-a725-047b0e8c3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1b9b99-9cf5-454c-83ec-a12ddab379e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\meama\\Documents\\A5\\AAI_Machine_Learning_for_NLP\\Project2\\df_new_format_abs_summarized2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29350b33-8e16-4cd1-bbcf-fde734878134",
   "metadata": {},
   "source": [
    "## Train T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adae84f-96c5-4181-8351-359a84be1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Prepare your data (you'll need to format your data as input-output pairs)\n",
    "input_texts = [\"input text 1\", \"input text 2\"]\n",
    "target_texts = [\"target text 1\", \"target text 2\"]\n",
    "\n",
    "# Encode the inputs and targets\n",
    "input_encodings = tokenizer(input_texts, padding=True, return_tensors=\"pt\")\n",
    "target_encodings = tokenizer(target_texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Define a training loop (this is highly simplified)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=input_encodings['input_ids'], \n",
    "                    labels=target_encodings['input_ids'])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./my_finetuned_t5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
